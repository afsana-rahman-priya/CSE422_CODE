# -*- coding: utf-8 -*-
"""10_19301181_AFSANA RAHMAN PRIYA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ekqyCwQ5jy854F0Zx8hNr0Moobr7Y7qN
"""



"""## CSE 422 Introduction to Data Preprocessing
---

### What are the advantages of preprocessing the data before applying on machine learning algorithm?

"The biggest advantage of pre-processing in ML is to improve **generalizablity** of your model. Data for any ML application is collected through some ‘sensors’. These sensors can be physical devices, instruments, software programs such as web crawlers, manual surveys, etc. Due to hardware malfunctions, software glitches, instrument failures, amd human errors, noise and erroneous information may creep in that can severely affect the performance of your model. Apart from **noise**, there are several **redundant information** that needs to be removed. For e.g. while predicting whether it rains tomorrow or not, age of the person is irrelevant. In terms of text processing, there are several stop words that may be redundant for the analysis. Lastly, there may be several **outliers** present in your data, due to the way data is collected that may need to be removed to improve the performance of the classifiers." 
                                    
                                            -Shehroz Khan, ML Researcher, Postdoc @U of Toronto

Some Data Preprocessing Techniques:

* Deleting duplicate and null values
* Imputation for missing values
* Handling Categorical Features
* Feature Normalization/Scaling
* Feature Engineering
* Feature Selection
"""

#importing necessary libraries
import pandas as pd
import numpy as np

"""#Removing Null values / Handling Missing data



"""

leaf = pd.read_csv('/content/leaf_dataset.csv')
leaf.head(341)

leaf.shape

leaf.isnull().sum()

"""### Imputing missing Values"""

from sklearn.impute import SimpleImputer

impute = SimpleImputer(missing_values=np.nan, strategy='mean')

impute.fit(leaf[['Elongation']])
leaf['Elongation'] = impute.transform(leaf[['Elongation']])

impute.fit(leaf[['Maximal Indentation Depth']])
leaf['Maximal Indentation Depth'] = impute.transform(leaf[['Maximal Indentation Depth']])

impute.fit(leaf[['Lobedness']])
leaf['Lobedness'] = impute.transform(leaf[['Lobedness']])

impute.fit(leaf[['Average Contrast']])
leaf['Average Contrast'] = impute.transform(leaf[['Average Contrast']])

leaf.isnull().sum()

"""## Standardizing Data

## Feature Scaling

## Why do we need to scale our data?
* If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset and make the estimator unable to learn from other features correctly, i.e. our learner might give more importance to features with high variance, which is not something we want happening in our model.

The following are a few different types of Scalers:

**MinMax Scaler:** 

Scales values to a range between 0 and 1 if no negative values, and -1 to 1 if there are negative values present.

$$\frac{X - X_{min}}{X_{max} - X_{min}}$$

where, 

 $$X\space is\space a\space feature\space value.$$ 
 $$X_{min} \space and \space X_{max} \space are \space corresponding \space feature's \space min \space and \space max \space values. $$


**Standard Scaler:**

$$\frac{X - mean}{\sigma}$$
where,
$$\sigma = standard \space deviation $$ 

**Robust Scaler:**

Uses statistics that are robust to outliers

$$\frac{X - median}{IQR}$$

where, 

$$ IQR = Inter\space Quartile\space Range = Q_3 - Q_1 $$

Sklearn library provides functions for different scalers by which we can easily scale our data.
"""

from sklearn.model_selection import train_test_split

data = leaf.drop('Class(species)', axis= 1)
target = leaf['Class(species)']

X_train, X_test, y_train, y_test = train_test_split(data, target, random_state = 1)

print(X_train.shape)
print(X_test.shape)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(copy=True, feature_range=(0, 1))
scaler.fit(X_train)

# transform data
X_train_scaled = scaler.transform(X_train)

print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))

"""We can see that after Min-Max Scaling all the values are in the range [0,1]"""

print("per-feature minimum after scaling:\n {}".format(X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(X_train_scaled.max(axis=0)))

# transform test data
X_test_scaled = scaler.transform(X_test)

"""## Effect of using MinMax Scaler:

###Accuracy without scaling
"""

from sklearn.neighbors import KNeighborsClassifier
X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)
knn=KNeighborsClassifier()

knn.fit(X_train, y_train)

print("Test set accuracy: {:.2f}".format(knn.score(X_test, y_test)))

"""### We can see that accuracy improves if we train on scaled data."""

# preprocessing using 0-1 scaling
scaler = MinMaxScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)

X_test_scaled = scaler.transform(X_test)

#train
knn.fit(X_train_scaled, y_train)

# scoring on the scaled test set
print("Scaled test set accuracy: {:.2f}".format(knn.score(X_test_scaled, y_test)))

"""## Feature Selection

### Checking for correlated features

We may use the following heatmap to find out the correlation between each of the features in a dataset. If a certain feature is highly correlated with more than one feature, we may choose to drop that feature (in this case it is *flavanoids*) because it will affect our model in a similar way as the other two features (and thus will prove to redundant). Correlation between two features may be found using the color gradient shown on the right.
"""

leaf.info()

leaf.head(341)

leaf_corr=leaf.corr()

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize = (20,15))
sns.heatmap(leaf_corr, cmap = 'coolwarm', annot=True)

plt.show()

leaf.corr()

def correlation(dataset,threshold):
  col_corr=set()
  corr_matrix=dataset.corr()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
      if abs(corr_matrix.iloc[i,j])> threshold:
        colname=corr_matrix.columns[i]
        col_corr.add(colname)
  return col_corr

# Take a minute to find the columns where the correlation value is greater than 0.75 at least twice
corr_features=correlation(leaf,0.75)
len(set(corr_features))
print('Number of correlated features: ', len(set(corr_features)))

print('correlated features are:', corr_features)

# Drop those columns from the DataFrame where the correlation value is greater than 0.75 at least twice 
to_drop = corr_features  

leaf = leaf.drop(to_drop, axis=1)


leaf.head(341)

"""## Split the dataset into features and labels

### Use your intuition to determine which column indicates the labels.
"""

features=leaf.drop('Class(species)',axis=1)
features.shape
print(features)


label = leaf['Class(species)']
label.shape
print(label)

"""## **Summary:**

Basic Pipeline for solving a ML project:

1. Read in Dataset

2. Get to know your dataset using data vizualisation and other techniques

3. Preprocess your dataset:

  * remove/impute null values
  * remove outliers
  * feature scaling
  * feature engineering
  * feature selection

4. train/test split
5. choose and build (number of) machine learning algorithm
5. train model on training data
6. make prediction on test data
7. evaluate performance on test data
8. visualization of your results

#**Support Vector Machine (SVM), Neural Network (Multilayer Perceptron Classifier) and Random Forest**

##3. Support Vector Machine (SVM), Neural Network (Multilayer Perceptron Classifier) and Random Forest are three very popular machine learning classifiers. Divide the dataset into 8:2 train-test split and perform Support Vector Machine, Neural Network (MLPClassifier) and Random Forest on it using sklearn library. In the previous assignment, you have already used Logistic Regression and decision tree classifiers from the sklearn library. Just change the imports and the function calls to use other classifiers. Take a look at the sklearn documentation for further information.
"""

#Prepare the training set

# X = feature values, all the columns except the first column
X = features

# y = target values, first column of the data frame
y = label

#Split the data into 80% training and 20% testing or Use 8:2 train-test split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""###Perform Support Vector Machine"""

from sklearn.svm import SVC
svc= SVC(kernel="linear")
svc.fit(x_train, y_train)

"""#### Evaluating the SVC model"""

print("Training accuracy of the SVC model is {:.2f}".format(svc.score(x_train, y_train)))
print("Testing accuracy of the SVC model is {:.2f}".format(svc.score(x_test, y_test)))

pre_svc_train_accuracy=svc.score(x_train, y_train)
pre_svc_test_accuracy=svc.score(x_test, y_test)

"""###Perform Neural Network (MLPClassifier) """

from sklearn.neural_network import MLPClassifier
nnc= MLPClassifier(activation="relu",hidden_layer_sizes=(7),max_iter=10000)
nnc.fit(x_train, y_train)

"""###Evaluating the NNC model"""

print("Training accuracy of the NNC model is {:.2f}".format(nnc.score(x_train, y_train)))
print("Testing accuracy of the NNC model is {:.2f}".format(nnc.score(x_test, y_test)))

pre_nnc_train_accuracy=nnc.score(x_train, y_train)
pre_nnc_test_accuracy=nnc.score(x_test, y_test)

"""##Perform Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rfc= RandomForestClassifier(n_estimators=50)
rfc.fit(x_train, y_train)

"""###Evaluating the RFC model"""

print("Training accuracy of the RFC model is {:.2f}".format(rfc.score(x_train, y_train)))
print("Testing accuracy of the RFC model is {:.2f}".format(rfc.score(x_test, y_test)))

pre_rfc_train_accuracy=rfc.score(x_train, y_train)
pre_rfc_test_accuracy=rfc.score(x_test, y_test)

"""##4. Perform dimensionality reduction using PCA. Reduce the number of feature vectors into half (e.g. if your dataset has 10 columns, after applying PCA it should have 5 columns)"""

from sklearn.decomposition import PCA 
pca = PCA(n_components=3)
pca.fit(x_train)
x_train_pca = pca.transform(x_train)
x_test_pca = pca.transform(x_test)

"""##5.Apply Support Vector Machine, Neural Network (MLPClassifier) and Random Forest again on the reduced dataset."""

#SVC after PCA
from sklearn.svm import SVC
svc= SVC(kernel="linear")
svc.fit(x_train_pca, y_train)

#Evaluating the SVC model after performing PCA
print("Training accuracy of the SVC model after performing PCA is {:.2f}".format(svc.score(x_train_pca, y_train)))
print("Testing accuracy of the SVC model after performing PCA is {:.2f}".format(svc.score(x_test_pca, y_test)))
post_svc_train_accuracy=svc.score(x_train_pca, y_train)
post_svc_test_accuracy=svc.score(x_test_pca, y_test)

#Neural Network after PCA
from sklearn.neural_network import MLPClassifier
nnc= MLPClassifier(activation="relu",hidden_layer_sizes=(7),max_iter=10000)
nnc.fit(x_train_pca, y_train)

#Evaluating the NNC model after performing PCA
print("Training accuracy of the NNC model after performing PCAis {:.2f}".format(nnc.score(x_train_pca, y_train)))
print("Testing accuracy of the NNC model after performing PCAis {:.2f}".format(nnc.score(x_test_pca, y_test)))
post_nnc_train_accuracy=svc.score(x_train_pca, y_train)
post_nnc_test_accuracy=svc.score(x_test_pca, y_test)

#Random Forest after PCA
from sklearn.ensemble import RandomForestClassifier
rfc= RandomForestClassifier(n_estimators=50)
rfc.fit(x_train_pca, y_train)

#Evaluating the RFC model after performing PCA
print("Training accuracy of the RFC model after performing PCA is {:.2f}".format(rfc.score(x_train_pca, y_train)))
print("Testing accuracy of the RFC model after performing PCA is {:.2f}".format(rfc.score(x_test_pca, y_test)))
post_rfc_train_accuracy=svc.score(x_train_pca, y_train)
post_rfc_test_accuracy=svc.score(x_test_pca, y_test)

"""##6. Compare the accuracy of the pre-PCA and post-PCA results. Plot them against each other in a bar graph."""

# Comparing Pre-PCA SVC & Post_PCA SVC of train set
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (5, 10))
plt.bar(['Pre-PCA SVC Train', 'Post-PCA SVC Train'], [pre_svc_train_accuracy,post_svc_train_accuracy], color =['royalblue', 'seagreen'], width = 0.5)
plt.ylabel("Acccuracy")
plt.xlabel("Types")
plt.title("Comparison")
plt.text(-0.07, pre_svc_train_accuracy+0.001, round(pre_svc_train_accuracy, 2) )
plt.text(0.92, post_svc_train_accuracy+0.001, round(post_svc_train_accuracy , 2) )
plt.show()

# Comparing Pre-PCA SVC & Post_PCA SVC of test set
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (5, 10))
plt.bar(['Pre-PCA SVC Test', 'Post-PCA SVC Test'], [pre_svc_test_accuracy,post_svc_test_accuracy], color =['royalblue', 'seagreen'], width = 0.5)
plt.ylabel("Acccuracy")
plt.xlabel("Types")
plt.title("Comparison")
plt.text(-0.07, pre_svc_test_accuracy+0.001, round(pre_svc_test_accuracy, 2) )
plt.text(0.92, post_svc_test_accuracy+0.001, round(post_svc_test_accuracy , 2) )
plt.show()

# Comparing Pre-PCA Neural Network & Post_PCA Neural Network train set
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (5, 10))
plt.bar(['Pre-PCA NNC Train', 'Post-PCA NNC Train'], [pre_nnc_train_accuracy,post_svc_train_accuracy], color =['royalblue', 'seagreen'], width = 0.5)
plt.ylabel("Acccuracy")
plt.xlabel("Types")
plt.title("Comparison")
plt.text(-0.07, pre_nnc_train_accuracy+0.01, round(pre_nnc_train_accuracy, 2) )
plt.text(0.92, post_nnc_train_accuracy+0.01, round(post_nnc_train_accuracy, 2) )
plt.show()

# Comparing Pre-PCA Neural Network & Post_PCA Neural Network test set
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (5, 10))
plt.bar(['Pre-PCA NNC Test', 'Post-PCA NNC Test'], [pre_nnc_test_accuracy,post_svc_test_accuracy], color =['royalblue', 'seagreen'], width = 0.5)
plt.ylabel("Acccuracy")
plt.xlabel("Types")
plt.title("Comparison")
plt.text(-0.07, pre_nnc_test_accuracy+0.003, round(pre_nnc_test_accuracy, 2) )
plt.text(0.92, post_nnc_test_accuracy+0.003, round(post_nnc_test_accuracy, 2) )
plt.show()

# Comparing Pre-PCA Random Forest & Post_PCA Random Forest train set
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (5, 10))
plt.bar(['Pre-PCA RFC Train', 'Post-PCA RFC Train'], [pre_rfc_train_accuracy,post_rfc_train_accuracy], color =['royalblue', 'seagreen'], width = 0.5)
plt.ylabel("Acccuracy")
plt.xlabel("Types")
plt.title("Comparison")
plt.text(-0.07, pre_rfc_train_accuracy+0.01, round(pre_rfc_train_accuracy, 2) )
plt.text(0.92, post_rfc_train_accuracy+0.01, round(post_rfc_train_accuracy, 2) )
plt.show()

# Comparing Pre-PCA Random Forest & Post_PCA Random Forest test set
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (5, 10))
plt.bar(['Pre-PCA RFC Test', 'Post-PCA RFC Test'], [pre_rfc_test_accuracy,post_rfc_test_accuracy], color =['royalblue', 'seagreen'], width = 0.5)
plt.ylabel("Acccuracy")
plt.xlabel("Types")
plt.title("Comparison")
plt.text(-0.07, pre_rfc_test_accuracy+0.01, round(pre_rfc_test_accuracy, 2) )
plt.text(0.92, post_rfc_test_accuracy+0.01, round(post_rfc_test_accuracy, 2) )
plt.show()